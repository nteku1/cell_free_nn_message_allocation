# -*- coding: utf-8 -*-
"""TAND_VARIATION_Copy of Solov_2_17_ver3data_Copy of TOPK_Copy of AVG_of ZEROS_VER3_Copy of SINGLE_COMBINE_TESTING_FIXED_Copy of Copy of Copy of EXP_rev_ Copy of USE_THIS_ACTUAL_SHAD_10_AP_3_UE_GENERALIZE_MULT_UE_Copy_7_16_TRYINGSTUFF_EXP_Copy_MULTI_Copy_ MORE_EXPERIMENTS_TESTING_CHECKING_EXP_Copy of EXP_TRY_DIFF_STRUCTURE_Copy of VER2_6_24_postmeeting_Copy_SIGMOID_WORKS_BUTCONSTANT_OUTPUT_ LATEST_COPY_FIXED_ddpg_sim_JUST_RATE_NO_ALPHA_updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ea0QfDaTeh1lhOgfZiZw44e6BRFIVHh-
"""


import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt


#SEED 2 UEs
tf.keras.utils.set_random_seed(15)
#tf.random.set_seed(13)   #have used 12 originally  - 13 gave the 0.06 result
#np.random.seed(22)

file1 = open('EFBACTUAL_RREDO_100meters_PRETTY_PLS_VER551_MULTIPLE_PATHS_LARGE_VER1_10AP_6_UE_50000SHAD.txt', 'r')
lines = file1.readlines()

num_APs = 10
num_user = 6 #3#20 #6 #3
pos = 0
raw_iq = []
complex_vec = []
complex_vec_total = []
count = 0
count2 = 0

for i in range(num_user):
  temp = lines[pos+i]
  temp = temp.strip()
  temp = temp.split(' ')
  raw_iq.append(temp)
  print(raw_iq[i])

for i in range(num_user):
  for jslack in raw_iq[i]:
  #i = float(i)
    if count % 2 == 0:
      tens = float(raw_iq[i][count])
    if  count % 2 == 1:
      complex_vec.append(complex(tens,float(raw_iq[i][count])))
      count2 = count2+ 1
    count = count + 1
  complex_vec_total.append(complex_vec)
  complex_vec = []
  count = 0
  count2 = 0

print(complex_vec_total[0])
print(complex_vec_total[1])

def converttt(lines,id):  
  raw_iq = []
  complex_vec = []
  complex_vec_total = []
  count = 0
  count2 = 0

  for i in range(num_user):
    temp = lines[id+i]
    temp = temp.strip()
    temp = temp.split(' ')
    raw_iq.append(temp)
    print(raw_iq[i])

  for i in range(num_user):
    for jslack in raw_iq[i]:
    #i = float(i)
      if count % 2 == 0:
        tens = float(raw_iq[i][count])
      if  count % 2 == 1:
        complex_vec.append(complex(tens,float(raw_iq[i][count])))
        count2 = count2+ 1
      count = count + 1
    complex_vec_total.append(complex_vec)
    complex_vec = []
    count = 0
    count2 = 0

  return complex_vec_total
  #print(complex_vec_total[0])
  #print(complex_vec_total[1])

freq = 1*10**6
B = 20*10**6
C = 20
S_u = 10**3
beta1 = 1
beta2 = 1 #100 for chan 1, 10 for chan 2, 1 for chan 3
p = 100 #100 #(mW)
ada_1 = 1
num_actions = num_APs
num_states = 2*num_APs # ust passing rate
#num_states = num_APs +num_APs#state requires alphas of each AP and channel gains between each AP and the user
'''
problem = "Pendulum-v0"
env = gym.make(problem)

num_states = env.observation_space.shape[0]
print("Size of State Space ->  {}".format(num_states))
num_actions = env.action_space.shape[0]
print("Size of Action Space ->  {}".format(num_actions))
'''

lower_bound = 0
upper_bound = 1
#upper_bound = env.action_space.high[0]
#lower_bound = env.action_space.low[0]

print("Max Value of Action ->  {}".format(upper_bound))
print("Min Value of Action ->  {}".format(lower_bound))

class Buffer:
    def __init__(self, buffer_capacity=100000, batch_size=64):
        # Number of "experiences" to store at max
        self.buffer_capacity = buffer_capacity
        # Num of tuples to train on.
        self.batch_size = batch_size

        # Its tells us num of times record() was called.
        self.buffer_counter = 0

        # Instead of list of tuples as the exp.replay concept go
        # We use different np.arrays for each tuple element
        ######self.state_buffer = np.zeros((self.buffer_capacity, num_states))
        self.state_buffer = np.zeros((self.buffer_capacity, num_user, num_states))
        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))
        self.reward_buffer = np.zeros((self.buffer_capacity, num_APs))
        #self.reward_buffer = np.zeros((self.buffer_capacity, 1))
        ########self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))
        self.next_state_buffer = np.zeros((self.buffer_capacity, num_user, num_states))
        self.GAINs = np.zeros((self.buffer_capacity, num_user,num_APs))
        #self.products = np.zeros((self.buffer_capacity, num_APs))
        #self.SINRs = np.zeros((self.buffer_capacity, num_APs))

    # Takes (s,a,r,s') obervation tuple as input
    def record(self, obs_tuple):
        # Set index to zero if buffer_capacity is exceeded,
        # replacing old records
        index = self.buffer_counter % self.buffer_capacity

        self.state_buffer[index] = obs_tuple[0]
        self.action_buffer[index] = obs_tuple[1]
        self.reward_buffer[index] = obs_tuple[2]
        self.next_state_buffer[index] = obs_tuple[3]
        self.GAINs[index] = obs_tuple[4]
        #self.products[index]= obs_tuple[4]
        #self.SINRs[index]= obs_tuple[5]

        self.buffer_counter += 1

    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows
    # TensorFlow to build a static graph out of the logic and computations in our function.
    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.
    #@tf.function
    def update(
        self, state_batch, action_batch, reward_batch, next_state_batch,gain_critic
    ):
        # Training and updating Actor & Critic networks.
        # See Pseudo Code.
         # TRY TO DO DIFFERENCE OF SHUFFLED ALPHAS???
        
        with tf.GradientTape() as tape:
            final_lats = []
            cccc = 0
            id = 0
            for i in state_batch:
              #prc = product_critic[cccc]
              #rc = rate_critic[cccc]
              #lats = []
              comp_lats = []
              trans_lats = []
              critic_value = tf.zeros([num_APs])
              anti_indices = []
              anti_update = []
              copy_crit_indices = []


              for jslack in i:
                temp_fw = critic_model((tf.expand_dims(jslack, 0)))[0]
                temp_indices = np.argsort(temp_fw)[6:]  #3
                temp_count = 0
                
                for isl in range(len(temp_fw)):
                  if temp_count not in temp_indices:
                    temp_fw = tf.tensor_scatter_nd_update(temp_fw, [[temp_count]], [0])
                  elif temp_fw[temp_count]<0:
                    temp_fw = tf.tensor_scatter_nd_update(temp_fw, [[temp_count]], [temp_fw[temp_count]*-1])
                    #temp_fw = tf.tensor_scatter_nd_update(temp_fw, [[temp_count]], [0])
                  temp_count += 1
                
                crit_indices = tf.experimental.numpy.where(temp_fw == 0)
                copy_crit_indices.append(crit_indices[0])
                crit_list = []
                cn = 0

                for ist in range(len(crit_indices[0])):
                  crit_list.append([])
                  for istt in range(1):
                    crit_list[ist].append(crit_indices[0][istt + cn])
                  cn += 1

                anti_indices.append(crit_list)
                update_list = []

                for upp in range(len(crit_indices[0])):
                  #update_list.append(crit_indices[0][upp*-1])
                  update_list.append(0)
                anti_update.append(update_list)
                
                '''
                new_critic_value = tf.tensor_scatter_nd_update(temp_fw, crit_list, update_list)
                new_critic_value = new_critic_value/tf.math.reduce_sum(new_critic_value)
                critic_value = tf.concat([critic_value, new_critic_value], axis=0)
                '''
                #critic_value = tf.concat([critic_value, temp_fw/tf.math.reduce_sum(temp_fw)], axis=0)

                if len(crit_list) != 0:
                  new_critic_value = tf.tensor_scatter_nd_update(temp_fw, crit_list, update_list)
                  new_critic_value = new_critic_value/tf.math.reduce_sum(new_critic_value)
                  critic_value = tf.concat([critic_value, new_critic_value], axis=0)
                else:
                  #critic_value = tf.concat([critic_value, temp_fw], axis=0)
                  #line below is in the case of softmax not being the ouput layer
                  critic_value = tf.concat([critic_value, temp_fw/tf.math.reduce_sum(temp_fw)], axis=0)





              
              critic_value = critic_value[num_APs:]
              sum_temptf = tf.zeros([num_APs],dtype='float64')
              sum_dentf = tf.zeros([num_APs],dtype='float64')

              for gan in range(num_user):
                alph = critic_value[gan*num_APs:num_APs + gan*num_APs]
                ########################product = alphasss*S_u
                omsquared = (1 - alph) ** 2
                squ = alph ** 2

                gain_vec = gain_critic[id][gan]
                if len(anti_indices[gan]) !=0 and len(anti_update[gan])!=0:
                  gain_vec = tf.tensor_scatter_nd_update(gain_vec, anti_indices[gan], anti_update[gan])
                gain_vec = tf.cast(gain_vec,dtype='float32')
                chansd = ((gain_vec) ** 2) #(np.absolute(gain_critic[id][gan]) ** 2)

                # num = ((B*np.log2(1+((p*ada_1)*(squareddd*chan_squared)))))
                ###########################den = 1+ ((p*ada_1)*(oneminussquared*chan_squared))
                dpp = ((p * ada_1) * (omsquared * chansd))

                tep = (((p * ada_1) * (squ * chansd)))
                ############################rate = ((B*np.log2((1+(((p*ada_1)*(squareddd*chan_squared))/den)))))
                dpp = tf.cast(dpp,dtype='float64')
                tep = tf.cast(tep,dtype='float64')

                sum_dentf = sum_dentf + dpp
                sum_temptf = sum_temptf + tep



                  # should do this if somehow channel becomes zero (i.e. shadow)
              frustrating_constant = tf.constant(1,dtype='float64')
              rat = ((B * np.log2((frustrating_constant + ((sum_temptf) / (1 + sum_dentf))))))
              id += 1


              for vin in range(num_user):
                if len(anti_indices[vin]) !=0 and len(anti_update[vin])!=0:
                    new_rat = tf.tensor_scatter_nd_update(rat, anti_indices[vin], anti_update[vin])
                else:
                    new_rat = rat
                new_rat = tf.cast(new_rat,dtype='float32')
                new_rat = new_rat[new_rat!=0]
                
                comp_lats.append(tf.cast(beta1*critic_value[vin*num_APs:num_APs+vin*num_APs]* S_u,tf.float32))
                trans_lats.append(tf.math.reduce_max(tf.cast((beta2 * (critic_value[vin*num_APs:num_APs+vin*num_APs][critic_value[vin*num_APs:num_APs+vin*num_APs]!=0]*S_u / new_rat)), tf.float32)))
                '''
                #Original latency
                first_term = tf.cast((beta1 * (critic_value[vin*num_APs:num_APs+vin*num_APs][critic_value[vin*num_APs:num_APs+vin*num_APs]!=0] * S_u * C) / freq), tf.float32)
                second_term = tf.cast((beta2 * (critic_value[vin*num_APs:num_APs+vin*num_APs][critic_value[vin*num_APs:num_APs+vin*num_APs]!=0]*S_u / new_rat)), tf.float32)
                lats.append(tf.math.reduce_max(first_term + second_term))
                '''
              #(tf.expand_dims(tf.convert_to_tensor(prev_state[jwiw], dtype=np.float32), 0))
              tf_actual_comp = tf.math.reduce_sum(comp_lats,axis=0)
              tf_actual_comp = tf.math.reduce_max((tf_actual_comp*C)/freq)
              tf_actual_trans = tf.math.reduce_mean(trans_lats)
              final_lats.append(tf_actual_comp + tf_actual_trans)
              cccc += 1
            
            #critic_loss = tf.math.reduce_max(final_lats) #??
            #######critic_loss = tf.math.reduce_sum(final_lats)
            critic_loss = tf.math.reduce_mean(final_lats)


        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)
        #critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)
        critic_optimizer.apply_gradients(
            zip(critic_grad, critic_model.trainable_variables)
        )

        '''
        for i in state_batch:
          print('state')
          print(i)
          critic_value = critic_model([i],training=True)  #i is the state
          print('critic value')
          print(critic_value)
          for valss in range(num_APs):
            if critic_value[valss] < 0:
              critic_value[valss] = 0#legal_action[val]*-1

          #re-normalize
          critic_value = critic_value/tf.math.reduce_sum(critic_value)
          print('normalized critic value')
          print(critic_value)
          critic_indices = np.where(critic_value==0)[0]
          new_critic_value = critic_value[critic_value != 0]
          #print('rate critic copy')
          #print(rate_critic[0].copy())
          new_rate_critic = rate_critic[0].copy()

          
          for i in critic_indices:  #setting rates where alpha = 0 to zero
            new_rate_critic[i] = 0 

          
          observing_rate_indices_critic = np.where(new_rate_critic==0)[0]
          observing_alphasss_indices_critic = np.where(critic_value==0)[0]
          tezerk_critic = len(observing_rate_indices_critic)
          tezerk2_critic = len(observing_alphasss_indices_critic)

          if tezerk_critic != tezerk2_critic:
            print('Size Damn it')
            print('Alphas')
            print(critic_value)
            quit()

          for excak_critic in range(tezerk_critic):
            if observing_rate_indices_critic[excak_critic] != observing_alphasss_indices_critic[excak_critic]:
              print('Alphas')
              print(critic_value)
              quit()
          



          ##TODO: FOR THE ALPHAS THAT ARE ZERO REMOVE THE CORRESPONDING INDICES FROM THE RATE EVEN IF NOT ZERO INSTEAD OF SEEING IF THEY MATCH
          new_rate_critic = new_rate_critic[new_rate_critic != 0]
          product_critic = critic_value*S_u

          product_critic = product_critic[product_critic!=0]

    




        first_term = tf.cast((beta1*(new_critic_value*S_u*C)/freq), tf.float64)
        lats = first_term + (beta2*(product_critic/new_rate_critic))
        lat_list = tf.concat([lat_list,lats])
        #lats = (beta1*(critic_value*S_u*C)/freq) + (beta2*(SINR_batch))
        '''
            
            


        

        


    # We compute the loss and update parameters
    def learn(self):
        # Get sampling range
        record_range = min(self.buffer_counter, self.buffer_capacity)
        # Randomly sample indices
        batch_indices = np.random.choice(record_range, self.batch_size)

        # Convert to tensors
        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])
        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])
        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])
        reward_batch = tf.cast(reward_batch, dtype=tf.float32) 
        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])
        #product_batch = tf.convert_to_tensor(self.products[batch_indices])
        #rate_batch = tf.convert_to_tensor(self.SINRs[batch_indices])
        gain_batch = tf.convert_to_tensor(self.GAINs[batch_indices])

        self.update(state_batch, action_batch, reward_batch, next_state_batch,gain_batch)



# This update target parameters slowly
# Based on rate `tau`, which is much less than one.
@tf.function
def update_target(target_weights, weights, tau):
    for (a, b) in zip(target_weights, weights):
        a.assign(b * tau + a * (1 - tau))

##orginal
'''
def get_actor():
    # Initialize weights between -3e-3 and 3-e3
    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)

    inputs = layers.Input(shape=(num_states,))
    out = layers.Dense(256, activation="tanh")(inputs)
    out = layers.BatchNormalization()(out)
    out = layers.Dense(256, activation="tanh")(out)
    out = layers.BatchNormalization()(out)
    outputs = layers.Dense(num_APs, activation="linear", kernel_initializer=last_init)(out) #originally 1 action

    # Our upper bound is 2.0 for Pendulum.
    #outputs = outputs * upper_bound
    model = tf.keras.Model(inputs, outputs)
    
    return model
'''


def get_critic():
    # State as input
    state_input = layers.Input(shape=(num_states))
    #state_out = layers.Dense(16, activation="tanh")(state_input)
    #state_out = layers.Dense(32, activation="tanh")(state_out)

    # Action as input
    ############################action_input = layers.Input(shape=(num_actions))
    #action_out = layers.Dense(32, activation="tanh")(action_input)

    # Both are passed through seperate layer before concatenating
    #concat = layers.Concatenate()([state_out, action_out])
    ################################concat = layers.Concatenate()([state_input, action_input])
    ############################ new
    out = layers.BatchNormalization()(state_input)   
    ##############################out = layers.BatchNormalization()(concat)    
    out = layers.Dense(128, activation="tanh")(out)  #400
    ##############################
    #out = layers.Dropout(0.3)(out)
    #######out = layers.Dense(256, activation="tanh")(concat)
    out = layers.BatchNormalization()(out)
    #out = layers.Dense(300, activation="softmax")(out)  #300
    #out = layers.BatchNormalization()(out)
    out = layers.Dense(64, activation="tanh")(out)  #300
    #out = layers.Dropout(0.3)(out)
    out = layers.BatchNormalization()(out)
    outputs = layers.Dense(num_APs,activation="linear")(out)  #originally linear/softmax??, sigmoid matches unif
    # Outputs single value for give state-action
    model = tf.keras.Model(state_input, outputs)
    
    return model

def policy(state):
    sampled_actions = tf.squeeze(critic_model(state))
    
    
    legal_action = sampled_actions.numpy()#/np.sum(sampled_actions.numpy()) 
    legal_indices = np.argsort(legal_action)[6:] #3
    id_count = 0
    for i in range(len(legal_action)):
      if id_count not in legal_indices:
        legal_action[id_count] = 0
      id_count += 1
    
    print('Removed indicies')
    print(legal_indices)
    for val in range(num_APs):
      if legal_action[val] < 0:
        legal_action[val] = legal_action[val]*-1 #0
    
    #re-normalize
    legal_action = legal_action/np.sum(legal_action) 
    
  
    
    


    #legal_action = softmax(legal_action)
    
    '''
    valll = np.max(legal_action)
    idddd = np.argmax(legal_action)
    update_val = np.random.uniform(0,valll)
    '''
    
   
    #strategy 1
    '''
    inttt_rand = np.random.randint(0,num_APs)
    legal_action[idddd] = legal_action[idddd] - update_val
    legal_action[inttt_rand] = legal_action[inttt_rand] + update_val
    '''

    #strategy 2
    '''
    legal_action[idddd] = legal_action[idddd] - update_val
    for_others_update_val = (update_val)/(num_APs-1)
    listttt = list(range(0,num_APs))
    listttt.pop(idddd)
    for jjj in listttt:
      legal_action[jjj] = legal_action[jjj] + for_others_update_val
    '''
    
 
    #strategy 3
    '''
    percentagesss = np.random.uniform(0,1,num_APs-1)
    percentagesss = percentagesss/np.sum(percentagesss)
    legal_action[idddd] = legal_action[idddd] - update_val
    percentagesss = percentagesss * update_val
    listttt = list(range(0,num_APs))
    listttt.pop(idddd)
    count = 0
    for jjj in listttt:
      if jjj ==  num_APs-1:
        legal_action[jjj] = legal_action[jjj] + percentagesss[idddd]
      else:
        legal_action[jjj] = legal_action[jjj] + percentagesss[jjj]
    '''
    
    
    
    



  

    

    return [np.squeeze(legal_action)]

def policy_testing(state):
    sampled_actions = tf.squeeze(critic_model(state))
    


    legal_action = sampled_actions.numpy()#/np.sum(sampled_actions.numpy())
    legal_indices = np.argsort(legal_action)[6:] #3
    id_count = 0
    for i in range(len(legal_action)):
      if id_count not in legal_indices:
        legal_action[id_count] = 0
      id_count += 1

    #legal_action = np.sort(legal_action)[7:]
    #adding this back in to help with nans


    print('Largest indicies')
    print(legal_indices)
    for val in range(num_APs):
      if legal_action[val] < 0:
        legal_action[val] = legal_action[val]*-1 #0
     
    #re-normalize
    legal_action = legal_action/np.sum(legal_action)
  
    
    
  
    
    


  
    #legal_action = softmax(legal_action)
    #################################legal_action = softmax(sampled_actions.numpy()) 
    #legal_action = np.exp(legal_action) + epsilonn
    #legal_action = np.random.dirichlet(legal_action) 

 
    '''
    HOWHTISWORK
    legal_action = np.random.dirichlet(legal_action)
    legal_action = np.exp(legal_action) + epsilonn
    '''
     

    #legal_action = np.log2(legal_action)
    #legal_action = legal_action/sum(legal_action)



    #legal_action = legal_action + noise #new new
    
    

 
    ###############legal_action = legal_action/np.sum(legal_action) 
    
    
    #legal_action = softmax(sampled_actions.numpy()+noise) #could try with random instead?
    #legal_action = sampled_actions

    '''
    afl = sampled_actions#+noise
    afl = np.clip(afl, lower_bound, upper_bound)

    legal_action = softmax(afl) #could try with random instead?
    '''

    return [np.squeeze(legal_action)]

std_dev = 0.2
#ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))  
#ou_noise = OUActionNoise(mean=np.zeros(num_APs), std_deviation=float(std_dev) * np.ones(num_APs))  

critic_model = get_critic()


# Learning rate for actor-critic models
#These worked!!!
#####critic_lr = 0.02   
#####actor_lr = 0.001     

#tanh tanh linear

critic_lr = 0.002 #0.0002
#critic_lr = 0.000002  #0.02 was working 0.0002 was worse    0.000002
#7/12 - 7:49
#actor_lr = 0.0001 #0.0001  brings zeros       0.01 makes it close to uniform    7/11/morning12 - did 0.001 got better results

critic_optimizer = tf.keras.optimizers.Adam(critic_lr) #adam

total_episodes = 1 #600 #300 #600 100!!!! #450 #90 20  200 works  wors   500 episodes


buffer = Buffer(200, 32) #200, 32 - 2 users was working!!!                50000  #200 -2:55, 64  128 works 10000 256  32 gave nans 512

# To store reward history of each episode
ep_reward_list = []
solo1 = []
solo2 = []
stop_now = 0
# To store average reward history of last few episodes
const_reward_list = []
unif_reward_list = []
new_reward = 0
best_alphas = np.zeros(shape=(1,num_APs))
#best_alphas = np.zeros(shape=(1,10))
b1  = 0
indexxx = 0
stop_now = 0

# Takes about 4 min to train
for ep in range(total_episodes):

    #prev_state = env.reset() ep*1000
    prev_state = np.zeros([num_states,])
    episodic_reward = 0
    alphasss = []
    
    if stop_now == 1:
      break
    
    


    for i in range(num_user):
      #rand_vecs = np.zeros(shape=(1,num_APs))
      #rand_vecs = rand_vecs + ((1/num_APs)*np.ones(shape=(1,num_APs)))
      rand_vecs = np.random.random(size=(1,num_APs))
      rand_vecs = rand_vecs/np.sum(rand_vecs)
      alphasss.append(rand_vecs[0])
    


    indexxx = ep*200
    
    #indexxx = ep*1000
    #print('Step')
    #print(indexxx)
    #print(200+ep*200)
    '''
    comp, fla = converttt(lines,0)
    prev_state[0:10] = fla
    prev_state[10:21] = alphasss
    '''

    #while indexxx < len(lines)-1:
    #while indexxx < 200 + ep*200:
    while indexxx < 200 + 100*200:
    #while indexxx < 15:
        # Uncomment this to see the Actor in action
        # But not in a python notebook.
        # env.render()
        #print(prev_state)
        #prev_state = np.asarray(prev_state).astype(np.float32)
        ############print(indexxx)   
        tf_prev_state = [[]]*num_user
        if indexxx > 0:
          print('Step')
          print(indexxx)
          for jwiw in range(num_user):
            tf_prev_state[jwiw] = (tf.expand_dims(tf.convert_to_tensor(prev_state[jwiw],dtype=np.float32), 0))
        else:
          for jwiw in range(num_user):
            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state[jwiw],dtype=np.float32), 0)

      
        actions = []
        #if indexxx == ep*1000: #1000: #0: 200
        if indexxx == ep*200:
          for ikea in range(num_user):
            actions.append(np.asarray(alphasss[ikea]))
        else:
          for ikea22 in range(num_user):
            actions.append(policy(tf_prev_state[ikea22])[0])
            #actions.append(policy(tf_prev_state[ikea22], ou_noise)[0])
          #action = softmax(tf_prev_state)
          ######action = action[0]
          
          #TO EXPERIMENT COMMENTING THIS OUT
          '''
          while 0 in action:
            action = policy(tf_prev_state, ou_noise)
            #action = softmax(tf_prev_state)
            action = action[0]
          '''
        comp_latencies_co = []
        trans_latencies_co = []
        comp_latencies_u_co = []
        trans_latencies_u_co = []
        comp_latencies_b_co = np.zeros(shape=(1,num_APs))
        trans_latencies_b_co = []
        latencies_co = []
        copy_of_latencies_co = []
        copy_of_products_co = []
        copy_of_SINRS_co = []
        copy_of_GAINS_co = np.absolute(complex_vec_total)
        latencies_u_co = []
        latencies_b_co = []
        sum_temp = np.zeros(shape=(1,num_APs))
        sum_den = np.zeros(shape=(1,num_APs))
        sum_temp_u = np.zeros(shape=(1,num_APs))
        sum_den_u = np.zeros(shape=(1,num_APs))
        sum_temp_b = []
        sum_den_b = []
        for swag in range(num_user):
          action = actions[swag]
    
          alphasss = action
          ########################product = alphasss*S_u
          oneminussquared = (1-alphasss)**2
          squareddd = alphasss**2
          chan_squared = (np.absolute(complex_vec_total[swag])**2)

          #num = ((B*np.log2(1+((p*ada_1)*(squareddd*chan_squared)))))
          ###########################den = 1+ ((p*ada_1)*(oneminussquared*chan_squared))
          den = ((p*ada_1)*(oneminussquared*chan_squared))

          temp = (((p*ada_1)*(squareddd*chan_squared)))
          ############################rate = ((B*np.log2((1+(((p*ada_1)*(squareddd*chan_squared))/den)))))


          sum_den[0] = sum_den[0] + den
          sum_temp[0] = sum_temp[0] + temp



          # should do this if somehow channel becomes zero (i.e. shadow)
        
        rate = ((B*np.log2((1+((sum_temp)/(1+sum_den))))))

        un_dB_sinr = ((sum_temp)/(1+sum_den))
        '''
        if indexxx > 0:
          print('Original')
          print(rate)
        '''
        #### CHecking ###
        for sew in range(num_user):
        
          alphasss_indices = np.where(actions[sew]==0)[0]
          print('Alphas Indices')
          print(alphasss_indices)

          new_rate = rate[0].copy()

          
          for i in alphasss_indices:  #setting rates where alpha = 0 to zero
            new_rate[i] = 0 
 
          print('Alphasss')
          print(actions[sew])
          new_alphasss = actions[sew][actions[sew] != 0]

          
          observing_rate_indices = np.where(new_rate==0)[0]
          observing_alphasss_indices = np.where(actions[sew]==0)[0]
          observing_alphasss_indices_extra = np.where(actions[sew]!=0)[0]
          tezerk = len(observing_rate_indices)
          tezerk2 = len(observing_alphasss_indices)

          if tezerk != tezerk2:
            print('Size Damn it')
            print('chan squared')
            print(chan_squared)
            print('Alphas')
            print(alphasss)
            print('Temp')
            print(temp)
            print('Den')
            print(den)
            print('Rate')
            print(rate[0])
            print('New Rate')
            print(new_rate)
            quit()

          for excak in range(tezerk):
            if observing_rate_indices[excak] != observing_alphasss_indices[excak]:
              print('Damn it')
              print(chan_squared)
              print('Alphas')
              print(alphasss)
              print('Temp')
              print(temp)
              print('Den')
              print(den)
              print('Rate')
              print(rate[0])
              print('New Rate')
              print(new_rate)
              quit()
          



          ##TODO: FOR THE ALPHAS THAT ARE ZERO REMOVE THE CORRESPONDING INDICES FROM THE RATE EVEN IF NOT ZERO INSTEAD OF SEEING IF THEY MATCH
          #temp_co_vec = new_rate.copy()
          temp_SINR_vec = new_rate.copy()
          temp_Prod_vec = new_rate.copy()
          new_rate = new_rate[new_rate != 0]
          product = actions[sew]*S_u

          product = product[product!=0]

        
          comp_latencies_co.append((beta1 *(actions[sew] * S_u ) ))
          trans_latencies_co.append(np.max(beta2*(product/new_rate)))

  

          #un_dB_sinr = (2**(new_rate/B)) - 1


        ###################################un_dB_sinr = ((((p*ada_1)*(squareddd*chan_squared))/den))




      # Recieve state and reward from environment.
      #state, reward, done, info = env.step(action)
          

        actual_comp_lat =  np.sum(comp_latencies_co,axis=0)#np.max(comp_latencies_co)
        actual_comp_lat =  np.max((actual_comp_lat*C)/freq)
        actual_trans_lat = np.mean(trans_latencies_co)
        final_rewards = actual_comp_lat + actual_trans_lat
          #########total_latency = (beta1*(new_alphasss*S_u*C)/freq) + (beta2*(product/new_rate))
          #total_latency = (beta2*(product/new_rate))
          ###########latencies_co.append(total_latency)
          
          
          
        '''
        final_rewards = []
        for fight in range(num_user):
          final_rewards.append(np.max(latencies_co[fight]))
        '''

        #xx1 = np.max(latencies_co[0])
        #xx2 = np.max(latencies_co[1])
        #reward =np.mean([xx1, xx2]) 
        '''
        if indexxx == ep*200:
          reward =np.mean([xx1, xx2]) 
          old_reward = reward
        else:
          reward =np.mean([xx1, xx2]) 
          reward = reward - old_reward
          old_reward = reward
        '''
          
        reward = (final_rewards)
        #reward = np.max([xx1, xx2]) 
        #reward =np.sum([xx1, xx2]) 

        #####Trying this out - 6PM
        #final_rewards = []
        #final_rewards.append(-1*xx1)
        #final_rewards.append(-1*xx2)
        #solo1.append(xx1)
        #solo2.append(xx2)
        


        #12/26 NOW SUPERVISED SO BEGINNING TO REMOVE -1
        #reward= -1*reward

      



        # End this episode when `done` is True
        #if done:
        #    break

        #prev_state = state
        #ep_reward_list.append(episodic_reward)
 
        ##### UNIFORM
        unif_actions = []
        for delicious in range(num_user):
          #unif_alphasss = 1/num_APs*np.ones([1,num_APs]) 
          #unif_alphasss = 1/10*np.ones([1,10]) 
          unif_alphasss = 1/4*np.ones([1,10])[0]
          xcel = np.random.choice(10,4,replace=False)
          for slap in range(len(unif_alphasss)):
            if slap not in xcel:
              unif_alphasss[slap] = 0
          unif_actions.append(unif_alphasss)

          product_u = unif_alphasss*S_u
          oneminussquared_u = (1-unif_alphasss)**2
          squareddd_u = unif_alphasss**2
          chan_squared_u = (np.absolute(complex_vec_total[delicious])**2)

          #num = ((B*np.log2(1+((p*ada_1)*(squareddd*chan_squared)))))
          ########################den_u = 1+ ((p*ada_1)*(oneminussquared_u*chan_squared_u))
          den_u = ((p*ada_1)*(oneminussquared_u*chan_squared_u))
          temp_u = (((p*ada_1)*(squareddd_u*chan_squared_u)))


          sum_den_u[0] = sum_den_u[0] + den_u
          sum_temp_u[0] = sum_temp_u[0] + temp_u
          comp_latencies_u_co.append(beta1*(unif_alphasss*S_u))
          #####trans_latencies_u_co.append(np.max(beta2*(product_u/rate_u)))

          #########rate_u = ((B*np.log2((1+(((p*ada_1)*(squareddd_u*chan_squared_u))/den_u)))))

          
        rate_u = ((B*np.log2((1+((sum_temp_u)/(1+sum_den_u))))))   
        actual_comp_lat_u =  np.sum(comp_latencies_u_co,axis=0)#np.max(comp_latencies_co)
        actual_comp_lat_u = np.max((actual_comp_lat_u*C)/freq)
        

        for sewn in range(num_user):
          new_rate_u = rate_u[0].copy()
          zero_u_indic = np.where(unif_actions[sewn] == 0)[0]

          for i in zero_u_indic:  # setting rates where alpha = 0 to zero
              new_rate_u[i] = 0


          new_alphasss_u = unif_actions[sewn][unif_actions[sewn] != 0]

          observing_rate_indices = np.where(new_rate_u == 0)[0]
          observing_alphasss_indices = np.where(unif_actions[sewn] == 0)[0]
          observing_alphasss_indices_extra = np.where(unif_actions[sewn] != 0)[0]
          tezerk = len(observing_rate_indices)
          tezerk2 = len(observing_alphasss_indices)

          if tezerk != tezerk2:
              print('Size Damn it')
              quit()

          for excak in range(tezerk):
              if observing_rate_indices[excak] != observing_alphasss_indices[excak]:
                  print('Damn it')
                  quit()

          new_rate_u = new_rate_u[new_rate_u != 0]
          product_u = new_alphasss_u * S_u
          product_u = product_u[product_u != 0]
          trans_latencies_u_co.append(np.max(beta2*(product_u/new_rate_u)))


        actual_trans_lat_u = np.mean(trans_latencies_u_co)
        #total_latency_u = (beta1*(unif_alphasss*S_u*C)/freq) + (beta2*(product_u/rate_u))
        #total_latency_u =  (beta2*(product_u/rate_u))
        #because alphas are always uniform for both users, can just multiply reward by 2. 

        reward_u = actual_trans_lat_u + actual_comp_lat_u#np.max(total_latency_u[0])   #2*
        '''
        if indexxx == ep*200:
          reward_u = np.max(total_latency_u[0])  
          old_reward_u = reward_u
        else:
          reward_u = np.max(total_latency_u[0])   
          reward_u = reward_u - old_reward_u
          old_reward_u = reward_u
        '''
          



        '''
        latencies_u_co.append(total_latency_u[0])
        xx1_u = np.max(latencies_u_co[0])
        xx2_u = np.max(latencies_u_co[1])
        reward_u =np.sum([xx1_u, xx2_u]) 
       '''
        
 
        
        unif_reward_list.append(reward_u)

        best_channel_gains = []
        #### BEST Method
        for great in range(num_user):
          best_id = np.argmax(np.abs(complex_vec_total)[great])
          best_channel_gains.append(best_id)
          best_alphasss = 1

          product_b = best_alphasss*S_u
          oneminussquared_b = (1-best_alphasss)**2
          squareddd_b = best_alphasss**2

          chan_squared_b = (np.absolute(complex_vec_total[great][best_id])**2)

          #num = ((B*np.log2(1+((p*ada_1)*(squareddd*chan_squared)))))
          #####################den_b = 1+ ((p*ada_1)*(oneminussquared_b*chan_squared_b))
          den_b = ((p*ada_1)*(oneminussquared_b*chan_squared_b))
          temp_b = (((p*ada_1)*(squareddd_b*chan_squared_b)))
          sum_den_b.append(den_b)
          sum_temp_b.append(temp_b)
          #####################rate_b = ((B*np.log2((1+(((p*ada_1)*(squareddd_b*chan_squared_b))/den_b)))))
        
        duplicates = []
        best_sums = []
        for solov in best_channel_gains:
          comp_latencies_b_co[0][solov] = comp_latencies_b_co[0][solov] + (beta1*(best_alphasss*S_u))
          if solov in duplicates:
            continue
          else:
            matches = [de for de,se in enumerate(best_channel_gains) if se==solov]
            #timeagain = np.argwhere(best_channel_gains == j)[0]
            #if more than 1 sum up the indices
            temp_sum_den_b = 0
            temp_sum_temp_b = 0
          for iterating in matches:
            temp_sum_den_b = temp_sum_den_b + sum_den_b[iterating]
            temp_sum_temp_b = temp_sum_temp_b + sum_temp_b[iterating]
          

          best_sums.append(temp_sum_temp_b)
          #delete from best_gains
          duplicates.append(solov)
          #duplicates.append(timeagain[0])
      
        for xv in range(len(best_sums)):
          copy_sums = np.asarray(best_sums.copy())
          copy_sums = np.delete(copy_sums,xv)
          #copy_sums.pop(xv)
          if len(copy_sums) == 0:
            copy_sums = 0.0
          rate_b = ((B*np.log2((1+best_sums[xv]/(1+copy_sums)))))
          #comp_latencies_b_co[0][duplicates[xv]] = comp_latencies_b_co[0][duplicates[xv]] + ((beta1*(best_alphasss*S_u*C)/freq))
          trans_latencies_b_co.append((beta2*(product_b/rate_b)))

        #actual_comp_lat_b =  np.sum(comp_latencies_b_co[0],axis=0)
        #actual_comp_lat_b = np.max((actual_comp_lat_b*C)/freq)
        actual_comp_lat_b = np.max((comp_latencies_b_co[0] * C) / freq)
        actual_trans_lat_b = np.mean(trans_latencies_b_co)
        final_rewards_b = actual_comp_lat_b + actual_trans_lat_b


        '''
        if best_channel_gains[0] == best_channel_gains[1]: # if same AP is chosen for both add them together. 
          sum_den_bb = np.sum(sum_den_b)
          sum_temp_bb = np.sum(sum_temp_b) #need to add for same alpha
          rate_b = ((B*np.log2((1+((sum_temp_bb)/(1+sum_den_bb))))))
          total_latency_b = (beta1*(best_alphasss*S_u*C)/freq) + (beta2*(product_b/rate_b))
          reward_b = total_latency_b #because now doing  for same alpha #2*total_latency_b #because same alpha
        else:  # if not they are sepearte entities
          for i in range(len(sum_den_b)):
            rate_b = ((B*np.log2((1+((sum_temp_b[i])/(1+sum_den_b[i]))))))
            total_latency_b = (beta1*(best_alphasss*S_u*C)/freq) + (beta2*(product_b/rate_b))
            latencies_b_co.append(total_latency_b)
          
          xx1_b = np.max(latencies_b_co[0])
          xx2_b = np.max(latencies_b_co[1])
          #reward_b =np.mean([xx1_b, xx2_b]) 
          reward_b = np.max([xx1_b, xx2_b]) 
          #reward_b =np.sum([xx1_b, xx2_b]) 
        '''
        reward_b = (final_rewards_b)

        '''
        if indexxx == ep*200: 
          old_reward_b = reward_b
        else:
          reward_b = reward_b - old_reward_b
          old_reward_b = reward_b
        '''
        
      

     
        const_reward_list.append(reward_b)
        
        indexxx = indexxx + num_user#2#1
        if indexxx < len(lines):
          complex_vec_total = converttt(lines,indexxx)
          state = []
          for iii in range(num_user):
            status =  np.empty((1, num_states), dtype=np.object)  #TODO: how to assign to None,30 or convert 1,1,30 to None,3
            for jwe in range(num_states):
              #status[0][jwe] = np.log2(np.absolute(complex_vec_total[iii][jwe]))
              #status[0][jwe] = un_dB_sinr[0][jwe]
              #status[0][jwe] = (un_dB_sinr[0][jwe])

              
              if jwe < num_APs:
                status[0][jwe] = un_dB_sinr[0][jwe]
              else:
                status[0][jwe] = (np.absolute(complex_vec_total[iii][jwe-10]))
              
            
            status= status[0].astype('float64')
              #############state[0][iii] = (un_dB_sinr[0][iii])
            state.append(status)
            
           
          
          #state = state[0].astype('float64')
        else:
          print('Stop')
          break
        
        
        
        #print('State')
        #print(state)
        #print('Alphasss')
        #print(actions)
        print('Reward')
        print(reward*-1)
        
        
        



        '''
        print('Computational latency')
        print((beta1*(alphasss*S_u*C)/freq)) 
        print('Transmission Latency')
        print(beta2*(product/rate))
        '''
        
        '''
        for ew in range(num_user):
           buffer.record((prev_state[ew], actions[ew], reward, state[ew],copy_of_SINRS_co[ew],copy_of_products_co[ew])) 
        '''
           #buffer.record((prev_state[ew], actions[ew], copy_of_latencies_co[ew], state[ew]))

           ####buffer.record((prev_state[ew], actions[ew], reward, state[ew])) #dont know if this should be -reward or not
           #buffer.record((prev_state[ew], actions[ew], reward, state[ew],actions[ew]*S_u,rate)) #dont know if this should be -reward or not
           #############buffer.record((prev_state[ew], action[ew], final_rewards[ew], state[ew])) #dont know if this should be -reward or not


        ############buffer.record((prev_state, alphasss, reward, state)) #dont know if this should be -reward or not
        
        episodic_reward = (reward)
        #12/23: NO LOGNER NEGATIVE
        #episodic_reward = (reward*-1)
        if (indexxx-num_user) != ep*200:
          buffer.record((prev_state, alphasss, reward, state,copy_of_GAINS_co)) #dont know if this should be -reward or not
        
        if indexxx > 102:  #34:
          buffer.learn()
      
        if indexxx == 3006:#4134:
          stop_now = 1
          break
        
      

        #indexxx = indexxx + 1
        prev_state = state
        ep_reward_list.append(episodic_reward)

        

    #ep_reward_list.append(episodic_reward)
    

    ## Mean of last 40 episodes
    ##avg_reward = np.mean(ep_reward_list[-40:])
    #print("Episode * {} * Avg Reward is ==> {}".format(ep, avg_reward))
    #vg_reward_list.append(avg_reward)

# Plotting graph
# Episodes versus Avg. Rewards
print(indexxx)
#count = b1/float(1000)
#print(count)


'''
#stats = np.arange(0, 0.04, 0.0001)
stats = np.arange(0, 0.1, 0.0001)
#stats = np.arange(2, 250, 2)
ep_cdf =np.zeros([1,len(stats)])[0]
unif_cdf =np.zeros([1,len(stats)])[0]
const_cdf =np.zeros([1,len(stats)])[0]
#solo1_cdf =np.zeros([1,len(stats)])[0]
#solo2_cdf =np.zeros([1,len(stats)])[0]
count = 0
for il in stats:
  #print('index ' + str(il))
  tata = len(np.argwhere(ep_reward_list<il))
  #print('count ' + str(tata))
  tata2 = len(np.argwhere(unif_reward_list<il))
  tata3 = len(np.argwhere(const_reward_list<il))
  tata4 = len(np.argwhere(solo1<il))
  tata5 = len(np.argwhere(solo2<il))
  ep_cdf[count] = tata/((200+ep*200)/num_user) #(1000+ep*1000)  (200+ep*200)
  unif_cdf[count] = tata2/((200+ep*200)/num_user)
  const_cdf[count] = tata3/((200+ep*200)/num_user)
  #solo1_cdf[count] = tata4/((200+ep*200)/2)
  #solo2_cdf[count] = tata4/((200+ep*200)/2)
  count = count + 1


plt.plot(stats,ep_cdf)
plt.plot(stats,unif_cdf)
plt.plot(stats,const_cdf)
#plt.plot(solo1_cdf)
#plt.plot(solo2_cdf)
plt.xlabel("Max Latency")
plt.ylabel("CDF")
plt.legend(['Distributed method w/DDPG', 'Uniform Method', 'Best Channel Method'])
plt.show()

print(ep_cdf)
print(unif_cdf)
'''
# Save the weights
critic_model.save_weights("sam260.h5")
#critic_model.save_weights("cell_critic.h5")

#target_actor.save_weights("cell_target_actor.h5")
#target_critic.save_weights("cell_target_critic.h5")

#getting new channel state info
file2 = open('EFBACTUAL_RREDO_100meters_PRETTY_PLS_VER551_MULTIPLE_PATHS_LARGE_VER1_10AP_6_UE_50000SHAD_part2.txt', 'r')
testlines = file2.readlines()

pos = 0
raw_iq = []
complex_vec2 = []
complex_vec_total_p2 = []
count = 0
count2 = 0

for i in range(num_user):
  temp = testlines[pos+i]
  temp = temp.strip()
  temp = temp.split(' ')
  raw_iq.append(temp)
  print(raw_iq[i])

for i in range(num_user):
  for jslack in raw_iq[i]:
  #i = float(i)
    if count % 2 == 0:
      tens = float(raw_iq[i][count])
    if  count % 2 == 1:
      complex_vec2.append(complex(tens,float(raw_iq[i][count])))
      count2 = count2+ 1
    count = count + 1
  complex_vec_total_p2.append(complex_vec2)
  complex_vec2 = []
  count = 0
  count2 = 0

print(complex_vec_total_p2[0])
print(complex_vec_total_p2[1])

#load weights -- need to understand what's needed here during testing phase
critic_model.load_weights("sam260.h5")


# To store reward history of each episode
ep_reward_list = []
# To store average reward history of last few episodes
const_reward_list = []
unif_reward_list = []
new_reward = 0
best_alphas = np.zeros(shape=(1,num_APs))
#best_alphas = np.zeros(shape=(1,10))
b1  = 0
indexxx = 0
total_episodes = 1
# Takes about 4 min to train
for ep in range(total_episodes):

    #prev_state = env.reset() ep*1000
    prev_state = np.zeros([num_states,])
    episodic_reward = 0
    alphasss = []
    for i in range(num_user):
      rand_vecs = np.random.random(size=(1,num_APs))
      rand_vecs = rand_vecs/np.sum(rand_vecs)
      alphasss.append(rand_vecs[0])
    
    #indexxx = ep*1000
    #print('Step')
    #print(indexxx)
    #print(200+ep*200)
    '''
    comp, fla = converttt(lines,0)
    prev_state[0:10] = fla
    prev_state[10:21] = alphasss
    '''

    #while indexxx < len(testlines)-1:
    while indexxx < len(testlines)-(num_user-1):
        # Uncomment this to see the Actor in action
        # But not in a python notebook.
        # env.render()
        #print(prev_state)
        #prev_state = np.asarray(prev_state).astype(np.float32)  
        tf_prev_state = [[]]*num_user
        if indexxx > 0:
          for jwiw in range(num_user):
            tf_prev_state[jwiw] = (tf.expand_dims(tf.convert_to_tensor(prev_state[jwiw],dtype=np.float32), 0))
        else:
          for jwiw in range(num_user):
            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state[jwiw],dtype=np.float32), 0)

      
        actions = []
        
        if indexxx == 0:
          for ikea in range(num_user):
            actions.append(np.asarray(alphasss[ikea]))
        else:
          for ikea22 in range(num_user):
            actions.append(policy_testing(tf_prev_state[ikea22])[0])
          #action = softmax(tf_prev_state)
          ######action = action[0]
          
          #TO EXPERIMENT COMMENTING THIS OUT
          '''
          while 0 in action:
            action = policy(tf_prev_state, ou_noise)
            #action = softmax(tf_prev_state)
            action = action[0]
          '''
        comp_latencies_co = []
        trans_latencies_co = []
        comp_latencies_u_co = []
        trans_latencies_u_co = []
        comp_latencies_b_co = np.zeros(shape=(1,num_APs))
        trans_latencies_b_co = []
        latencies_co = []
        latencies_u_co = []
        latencies_b_co = []
        sum_temp = np.zeros(shape=(1,num_APs))
        sum_den = np.zeros(shape=(1,num_APs))
        sum_temp_u = np.zeros(shape=(1,num_APs))
        sum_den_u = np.zeros(shape=(1,num_APs))
        sum_temp_b = []
        sum_den_b = []
        for swag in range(num_user):
          action = actions[swag]
    
          alphasss = action
          ########################product = alphasss*S_u
          oneminussquared = (1-alphasss)**2
          squareddd = alphasss**2
          chan_squared = (np.absolute(complex_vec_total_p2[swag])**2)

          #num = ((B*np.log2(1+((p*ada_1)*(squareddd*chan_squared)))))
          ###########################den = 1+ ((p*ada_1)*(oneminussquared*chan_squared))
          den = ((p*ada_1)*(oneminussquared*chan_squared))





          temp = (((p*ada_1)*(squareddd*chan_squared)))
          ############################rate = ((B*np.log2((1+(((p*ada_1)*(squareddd*chan_squared))/den)))))


          sum_den[0] = sum_den[0] + den
          sum_temp[0] = sum_temp[0] + temp



          # should do this if somehow channel becomes zero (i.e. shadow)
        
        rate = ((B*np.log2((1+((sum_temp)/(1+sum_den))))))


        un_dB_sinr = ((sum_temp)/(1+sum_den))
        '''
        if indexxx > 0:
          print('Original')
          print(rate)
        '''
        #### CHecking ###
        
        for sew in range(num_user):
        
          alphasss_indices = np.where(actions[sew]==0)[0]
          print('Alphas Indices')
          print(alphasss_indices)


          new_rate = rate[0].copy()

          
          for i in alphasss_indices:  #setting rates where alpha = 0 to zero
            new_rate[i] = 0 

          print('Alphasss')
          print(actions[sew])
          new_alphasss = actions[sew][actions[sew] != 0]

          
          observing_rate_indices = np.where(new_rate==0)[0]
          observing_alphasss_indices = np.where(actions[sew]==0)[0]
          tezerk = len(observing_rate_indices)
          tezerk2 = len(observing_alphasss_indices)

          if tezerk != tezerk2:
            print('Size Damn it')
            print('chan squared')
            print(chan_squared)
            print('Alphas')
            print(alphasss)
            print('Temp')
            print(temp)
            print('Den')
            print(den)
            print('Rate')
            print(rate[0])
            print('New Rate')
            print(new_rate)
            quit()

          for excak in range(tezerk):
            if observing_rate_indices[excak] != observing_alphasss_indices[excak]:
              print('Damn it')
              print(chan_squared)
              print('Alphas')
              print(alphasss)
              print('Temp')
              print(temp)
              print('Den')
              print(den)
              print('Rate')
              print(rate[0])
              print('New Rate')
              print(new_rate)
              quit()
          



          ##TODO: FOR THE ALPHAS THAT ARE ZERO REMOVE THE CORRESPONDING INDICES FROM THE RATE EVEN IF NOT ZERO INSTEAD OF SEEING IF THEY MATCH
          new_rate = new_rate[new_rate != 0]
          product = actions[sew]*S_u

          product = product[product!=0]
          

        
      

  

          #un_dB_sinr = (2**(new_rate/B)) - 1


          #un_dB_sinr = ((((p*ada_1)*(squareddd*chan_squared))/den))




      # Recieve state and reward from environment.
      #state, reward, done, info = env.step(action)

          comp_latencies_co.append((beta1 *(actions[sew] * S_u ) ))
          trans_latencies_co.append(np.max(beta2*(product/new_rate)))
          
          #total_latency = (beta1*(new_alphasss*S_u*C)/freq) +  (beta2*(product/new_rate))
          #total_latency =  (beta2*(product/new_rate))

          #latencies_co.append(total_latency)
          
        

        actual_comp_lat =  np.sum(comp_latencies_co,axis=0)#np.max(comp_latencies_co)
        actual_comp_lat = np.max((actual_comp_lat*C)/freq)
        actual_trans_lat = np.mean(trans_latencies_co)
        final_rewards = actual_comp_lat + actual_trans_lat
        #final_rewards = []
        '''
        for fight in range(num_user):
          final_rewards.append(np.max(latencies_co[fight]))
        '''

        #xx1 = np.max(latencies_co[0])
        #xx2 = np.max(latencies_co[1])
        #reward =np.mean([xx1, xx2]) 
        #reward = np.max([xx1,xx2])
        reward = final_rewards #np.max(final_rewards)

        
        if indexxx >0 and indexxx < len(testlines):
          ep_reward_list.append(reward)

        #reward =np.sum([xx1, xx2]) 






      



        # End this episode when `done` is True
        #if done:
        #    break

        #prev_state = state
        #ep_reward_list.append(episodic_reward)
 
        ##### UNIFORM
        ##### UNIFORM
        unif_actions = []
        for delicious in range(num_user):
          #unif_alphasss = 1/num_APs*np.ones([1,num_APs]) 
          #unif_alphasss = 1/10*np.ones([1,10]) 
          unif_alphasss = 1/4*np.ones([1,10])[0]
          xcel = np.random.choice(10,4,replace=False)
          for slap in range(len(unif_alphasss)):
            if slap not in xcel:
              unif_alphasss[slap] = 0
          unif_actions.append(unif_alphasss)
          product_u = unif_alphasss*S_u
          oneminussquared_u = (1-unif_alphasss)**2
          squareddd_u = unif_alphasss**2
          chan_squared_u = (np.absolute(complex_vec_total_p2[delicious])**2)

          #num = ((B*np.log2(1+((p*ada_1)*(squareddd*chan_squared)))))
          ########################den_u = 1+ ((p*ada_1)*(oneminussquared_u*chan_squared_u))
          den_u = ((p*ada_1)*(oneminussquared_u*chan_squared_u))
          temp_u = (((p*ada_1)*(squareddd_u*chan_squared_u)))


          sum_den_u[0] = sum_den_u[0] + den_u
          sum_temp_u[0] = sum_temp_u[0] + temp_u
          comp_latencies_u_co.append(beta1*(unif_alphasss*S_u))

          #########rate_u = ((B*np.log2((1+(((p*ada_1)*(squareddd_u*chan_squared_u))/den_u)))))

          
        rate_u = ((B*np.log2((1+((sum_temp_u)/(1+sum_den_u))))))   
        actual_comp_lat_u =  np.sum(comp_latencies_u_co,axis=0)#np.max(comp_latencies_co)
        actual_comp_lat_u = np.max((actual_comp_lat_u*C)/freq)

        for sewn in range(num_user):
          new_rate_u = rate_u[0].copy()
          zero_u_indic = np.where(unif_actions[sewn] == 0)[0]

          for i in zero_u_indic:  # setting rates where alpha = 0 to zero
              new_rate_u[i] = 0


          new_alphasss_u = unif_actions[sewn][unif_actions[sewn] != 0]

          observing_rate_indices = np.where(new_rate_u == 0)[0]
          observing_alphasss_indices = np.where(unif_actions[sewn] == 0)[0]
          observing_alphasss_indices_extra = np.where(unif_actions[sewn] != 0)[0]
          tezerk = len(observing_rate_indices)
          tezerk2 = len(observing_alphasss_indices)

          if tezerk != tezerk2:
              print('Size Damn it')
              print('Rate')
              print(new_rate_u)
              print('alphas')
              print(new_alphasss_u)
              quit()

          for excak in range(tezerk):
              if observing_rate_indices[excak] != observing_alphasss_indices[excak]:
                  print('Damn it')
                  print('Rate')
                  print(new_rate_u)
                  print('alphas')
                  print(new_alphasss_u)
                  quit()

          new_rate_u = new_rate_u[new_rate_u != 0]
          product_u = new_alphasss_u * S_u
          product_u = product_u[product_u != 0]
          trans_latencies_u_co.append(np.max(beta2*(product_u/new_rate_u)))

        actual_trans_lat_u = np.mean(trans_latencies_u_co)
        #total_latency_u = (beta1*(unif_alphasss*S_u*C)/freq) + (beta2*(product_u/rate_u))
        #total_latency_u =  (beta2*(product_u/rate_u))
        #because alphas are always uniform for both users, can just multiply reward by 2. 

        reward_u = actual_trans_lat_u + actual_comp_lat_u
        #total_latency_u = (beta1*(unif_alphasss*S_u*C)/freq) + (beta2*(product_u/rate_u))
        #total_latency_u = (beta2*(product_u/rate_u))
        #because alphas are always uniform for both users, can just multiply reward by 2. 
        #reward_u = 2*np.max(total_latency_u[0])  
        '''
        latencies_u_co.append(total_latency_u[0])
        xx1_u = np.max(latencies_u_co[0])
        xx2_u = np.max(latencies_u_co[1])
        reward_u =np.sum([xx1_u, xx2_u]) 
       '''
        
        ##NEW - 7/16
        if indexxx >0 and indexxx < len(testlines):
          unif_reward_list.append(reward_u)


        #### BEST Method
        best_channel_gains = []
        final_best_reward = []
        #### BEST Method
        for great in range(num_user):
          best_id = np.argmax(np.abs(complex_vec_total_p2)[great])
          best_channel_gains.append(best_id)
          best_alphasss = 1

          product_b = best_alphasss*S_u
          oneminussquared_b = (1-best_alphasss)**2
          squareddd_b = best_alphasss**2

          chan_squared_b = (np.absolute(complex_vec_total_p2[great][best_id])**2)

          #num = ((B*np.log2(1+((p*ada_1)*(squareddd*chan_squared)))))
          #####################den_b = 1+ ((p*ada_1)*(oneminussquared_b*chan_squared_b))
          den_b = ((p*ada_1)*(oneminussquared_b*chan_squared_b))
          temp_b = (((p*ada_1)*(squareddd_b*chan_squared_b)))
          sum_den_b.append(den_b)
          sum_temp_b.append(temp_b)
          #####################rate_b = ((B*np.log2((1+(((p*ada_1)*(squareddd_b*chan_squared_b))/den_b)))))
         
        duplicates = []
        best_sums = []
        for solovvv in best_channel_gains:
          comp_latencies_b_co[0][solovvv] = comp_latencies_b_co[0][solovvv] + (beta1*(best_alphasss*S_u))
          if solovvv in duplicates:
            continue
          else:
            matches = [deee for deee,see in enumerate(best_channel_gains) if see==solovvv]
            #timeagain = np.argwhere(best_channel_gains == j)[0]
            #if more than 1 sum up the indices
            #temp_sum_den_b = 0
            temp_sum_temp_b = 0
          for iterating in matches:
            #temp_sum_den_b = temp_sum_den_b + sum_den_b[iterating]
            temp_sum_temp_b = temp_sum_temp_b + sum_temp_b[iterating]
          
          best_sums.append(temp_sum_temp_b)
          #delete from best_gains
          duplicates.append(solovvv)
          #duplicates.append(timeagain[0])
        
        for xv in range(len(best_sums)):
          copy_sums = np.asarray(best_sums.copy())
          copy_sums = np.delete(copy_sums,xv)
          #copy_sums.pop(xv)
          if len(copy_sums) == 0:
            copy_sums = 0.0
          rate_b = ((B*np.log2((1+best_sums[xv]/(1+copy_sums)))))
          #comp_latencies_b_co[0][duplicates[xv]] = comp_latencies_b_co[0][duplicates[xv]] + ((beta1*(best_alphasss*S_u*C)/freq))
          trans_latencies_b_co.append((beta2*(product_b/rate_b)))

        #actual_comp_lat_b =  np.sum(comp_latencies_b_co[0],axis=0)
        #actual_comp_lat_b = np.max((actual_comp_lat_b*C)/freq)
        actual_comp_lat_b = np.max((comp_latencies_b_co[0] * C) / freq)
        actual_trans_lat_b = np.mean(trans_latencies_b_co)
        final_rewards_b = actual_comp_lat_b + actual_trans_lat_b
          #####total_latency_b = (beta1*(best_alphasss*S_u*C)/freq) + (beta2*(product_b/rate_b))
          #total_latency_b = (beta2*(product_b/rate_b))
          ######latencies_b_co.append(total_latency_b)


        '''


        if best_channel_gains[0] == best_channel_gains[1]: # if same AP is chosen for both add them together. 
          sum_den_bb = np.sum(sum_den_b)
          sum_temp_bb = np.sum(sum_temp_b) #need to add for same alpha
          rate_b = ((B*np.log2((1+((sum_temp_bb)/(1+sum_den_bb))))))
          total_latency_b = (beta1*(best_alphasss*S_u*C)/freq) + (beta2*(product_b/rate_b))
          reward_b = total_latency_b #because same alpha
        else:  # if not they are sepearte entities
          for i in range(len(sum_den_b)):
            rate_b = ((B*np.log2((1+((sum_temp_b[i])/(1+sum_den_b[i]))))))
            total_latency_b = (beta1*(best_alphasss*S_u*C)/freq) + (beta2*(product_b/rate_b))
            latencies_b_co.append(total_latency_b)
          
          xx1_b = np.max(latencies_b_co[0])
          xx2_b = np.max(latencies_b_co[1])
          #reward_b =np.mean([xx1_b, xx2_b]) 
          reward_b =np.max([xx1_b, xx2_b]) 
          #reward_b =np.sum([xx1_b, xx2_b]) 
        '''

        reward_b = (final_rewards_b)

      
       
         #NEW - 7/16
        if indexxx > 0 and indexxx < len(testlines):
          const_reward_list.append(reward_b)

        print('Step')
        print(indexxx)
        indexxx = indexxx + num_user #2#1
        #if indexxx < len(testlines):
        if indexxx < len(testlines) - num_user:   #6000 hardcode
          complex_vec_total_p2 = converttt(testlines,indexxx)
          state = []
          for iii in range(num_user):
            status =  np.empty((1, num_states), dtype=np.object)  #TODO: how to assign to None,30 or convert 1,1,30 to None,3
            for jwe in range(num_states):
              #status[0][jwe] = np.log2(np.absolute(complex_vec_total_p2[iii][jwe])) #un_dB_sinr[0][jwe]
              #status[0][jwe] = (un_dB_sinr[0][jwe])
              
              if jwe < num_APs:
                status[0][jwe] = un_dB_sinr[0][jwe]
              else:
                status[0][jwe] = (np.absolute(complex_vec_total_p2[iii][jwe-10]))
              #latencies_co[iii][jwe] #Go back to sinr instead??
              
            
            status= status[0].astype('float64')
              #############state[0][iii] = (un_dB_sinr[0][iii])
            state.append(status)
            
           
          
          #state = state[0].astype('float64')
        else:
          print('Stop')
          break
        
        
        '''
        print('State')
        print(state)
        '''
        '''
        print('Alphasss')
        print(action)
        '''
        print('Reward')
        print(reward)
        #print('Uniform reward')
        #print(reward_u)
        
        



        
        print('Computational latency')
        print((beta1*(new_alphasss*S_u*C)/freq)) 
        print('Transmission Latency')
        print(beta2*(product/new_rate))

        

        ############buffer.record((prev_state, alphasss, reward, state)) #dont know if this should be -reward or not
          
        #episodic_reward = (reward)
        
        #indexxx = indexxx + 1
        prev_state = state

        #ep_reward_list.append(episodic_reward)

        

    #ep_reward_list.append(episodic_reward)
    

    ## Mean of last 40 episodes
    ##avg_reward = np.mean(ep_reward_list[-40:])
    #print("Episode * {} * Avg Reward is ==> {}".format(ep, avg_reward))
    #vg_reward_list.append(avg_reward)

# Plotting graph
# Episodes versus Avg. Rewards
print(indexxx)
#count = b1/float(1000)
#print(count)


min_cdf_mean  = [0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000]

#CPU - 128 64 4134 REDO in same after fixes - consistent with tape - 3 Ues top 4 multiply by -1 VARIATION  999  stop now back in trial 2
stats22 = np.arange(0, 0.2, 0.001)
#stats22 = np.arange(5, 300, 5) #400
ep_cdf22 =np.zeros([1,len(stats22)])[0]
unif_cdf22 =np.zeros([1,len(stats22)])[0]
const_cdf22 =np.zeros([1,len(stats22)])[0]
count22 = 0
for il in stats22:
  tata22 = len(np.argwhere(ep_reward_list<=il))
  tata222 = len(np.argwhere(unif_reward_list<=il))
  tata223 = len(np.argwhere(const_reward_list<=il))
  ep_cdf22[count22] = tata22/len(ep_reward_list)#999#1000
  unif_cdf22[count22] = tata222/len(unif_reward_list)#999#1000
  const_cdf22[count22] = tata223/len(const_reward_list)#999#1000
  count22 = count22 + 1

#plt.xlim(0, 70)
plt.plot(stats22*1000,ep_cdf22)
plt.plot(stats22*1000,unif_cdf22)
plt.plot(stats22*1000,const_cdf22)
plt.plot(stats22*1000,min_cdf_mean)
#plt.plot(stats22,dqn_cdf)
plt.xlabel("Max Latency (ms)")
plt.ylabel("CDF")
plt.legend(['Neural Network', 'Uniform Allocation', 'Greedy Allocation','Interior Point'])
plt.show()

print('ep reward list')
print(ep_reward_list)
print('unif reward list')
print(unif_reward_list)
print('const reward list')
print(const_reward_list)
print('ep cdf22')
print(ep_cdf22)
print('unif cdf22')
print(unif_cdf22)
print('const cdf22')
print(const_cdf22)
